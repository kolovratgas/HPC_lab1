{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numba2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPH1VrJrP8q8pg26kSFOkUh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kolovratgas/HPC_labs/blob/main/lab2/Numba2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Juyoxheq3zp",
        "outputId": "d2c1dc01-3e22-4510-a443-37de735dfd48"
      },
      "source": [
        "!pip install numba"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba) (1.19.5)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.34.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eWL8Ineq_-o",
        "outputId": "e4dbb423-4d26-4e39-ea6b-815a9821aeec"
      },
      "source": [
        "from numba import njit, jit, int32, cuda\n",
        "import numba\n",
        "import numpy as np\n",
        "import time\n",
        "from time import time\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "%pylab inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['time', 'int32']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD6g965N64gm"
      },
      "source": [
        "x1 = 1_000\n",
        "x2 = 10_000\n",
        "x3 = 100_000\n",
        "x4 = 500_000\n",
        "x5 = 1_000_000\n",
        "a1 = np.random.randint(1, 100, x1)\n",
        "a2 = np.random.randint(1, 100, x2)\n",
        "a3 = np.random.randint(1, 100, x3)\n",
        "a4 = np.random.randint(1, 100, x4)\n",
        "a5 = np.random.randint(1, 100, x5)\n",
        "a_all = [a1, a2, a3, a4, a5]\n",
        "c1 = np.zeros(1, dtype = np.int32)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR35vW723QAc"
      },
      "source": [
        "threads_per_block = 32"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzvZD7934CJ7"
      },
      "source": [
        "**РАСЧЁТ НА GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDM6Zyjt3Yub"
      },
      "source": [
        "@cuda.jit\n",
        "def dev_vec_sum(a, c):\n",
        "  sh_b = cuda.shared.array(threads_per_block, int32)\n",
        "\n",
        "  thread_id = cuda.threadIdx.x\n",
        "  block_id = cuda.blockIdx.x\n",
        "  idx = thread_id + block_id * threads_per_block\n",
        "  sh_b[thread_id] = 0\n",
        "\n",
        "  if idx < a.shape[0]:\n",
        "    sh_b[thread_id] = a[idx]\n",
        "    cuda.syncthreads()\n",
        "    if cuda.threadIdx.x == 0:\n",
        "      sum_of_arr = 0\n",
        "      for i in range(threads_per_block):\n",
        "        sum_of_arr += sh_b[i]\n",
        "      cuda.atomic.add(c, 0, sum_of_arr)\n",
        "\n",
        "def DeviceHost(a, c):\n",
        "    dev_a = cuda.to_device(a)\n",
        "    dev_c = cuda.to_device(c)\n",
        "    dev_vec_sum[threads_per_block, threads_per_block](dev_a, dev_c)\n",
        "    c = dev_c.copy_to_host()\n",
        "    return c"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjR-1tYZ9LDp"
      },
      "source": [
        "def result_gpu(my_a):\n",
        "  for i in range(len(my_a)):\n",
        "    start = time.time()\n",
        "    DeviceHost(my_a[i], c1)\n",
        "    end = time.time() - start\n",
        "    print('Расчёт на GPU: ')\n",
        "    print(end)\n",
        "    print(f'При размере вектора: {len(my_a[i])}')\n",
        "    print('--------------------------------')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hppu8wcIBjNV"
      },
      "source": [
        "def cpu_vec_sum(a):\n",
        "  return np.sum(a)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSZevMW336fU"
      },
      "source": [
        "**РАСЧЁТ НА CPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDWHghHvB2AG"
      },
      "source": [
        "def result_cpu(my_a):\n",
        "  for i in range(len(my_a)):\n",
        "    start = time.time()\n",
        "    cpu_vec_sum(my_a[i])\n",
        "    end = time.time() - start\n",
        "    print('Расчёт на CPU: ')\n",
        "    print(end)\n",
        "    print(f'При размере вектора: {len(my_a[i])}')\n",
        "    print('--------------------------------')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZwwkQ-TCEu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273b392b-13eb-4c51-cbd6-9252b67c33fb"
      },
      "source": [
        "result_cpu(a_all)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Расчёт на CPU: \n",
            "0.0008549690246582031\n",
            "При размере вектора: 1000\n",
            "--------------------------------\n",
            "Расчёт на CPU: \n",
            "0.0015189647674560547\n",
            "При размере вектора: 10000\n",
            "--------------------------------\n",
            "Расчёт на CPU: \n",
            "0.00016880035400390625\n",
            "При размере вектора: 100000\n",
            "--------------------------------\n",
            "Расчёт на CPU: \n",
            "0.011291980743408203\n",
            "При размере вектора: 500000\n",
            "--------------------------------\n",
            "Расчёт на CPU: \n",
            "0.0010750293731689453\n",
            "При размере вектора: 1000000\n",
            "--------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ-FC_-a-Cuz",
        "outputId": "3cd765a8-38e1-49a0-d778-3a904bd3cb0f"
      },
      "source": [
        "result_gpu(a_all)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Расчёт на GPU: \n",
            "0.21477174758911133\n",
            "При размере вектора: 1000\n",
            "--------------------------------\n",
            "Расчёт на GPU: \n",
            "0.005827903747558594\n",
            "При размере вектора: 10000\n",
            "--------------------------------\n",
            "Расчёт на GPU: \n",
            "0.002980470657348633\n",
            "При размере вектора: 100000\n",
            "--------------------------------\n",
            "Расчёт на GPU: \n",
            "0.003986358642578125\n",
            "При размере вектора: 500000\n",
            "--------------------------------\n",
            "Расчёт на GPU: \n",
            "0.003559589385986328\n",
            "При размере вектора: 1000000\n",
            "--------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7hM8kvl30Lu"
      },
      "source": [
        "**РАСЧЁТ НА GPU БЕЗ УЧЁТА ПЕРЕНОСА НА ХОСТ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Ch1ZFYySZ7"
      },
      "source": [
        "@cuda.jit\n",
        "def dev_vec_sum_new(a, c):\n",
        "  sh_b = cuda.shared.array(threads_per_block, int32)\n",
        "\n",
        "  thread_id = cuda.threadIdx.x\n",
        "  block_id = cuda.blockIdx.x\n",
        "  idx = thread_id + block_id * threads_per_block\n",
        "  sh_b[thread_id] = 0\n",
        "\n",
        "  if idx < a.shape[0]:\n",
        "    sh_b[thread_id] = a[idx]\n",
        "    cuda.syncthreads()\n",
        "    if cuda.threadIdx.x == 0:\n",
        "      sum_of_arr = 0\n",
        "      for i in range(threads_per_block):\n",
        "        sum_of_arr += sh_b[i]\n",
        "      cuda.atomic.add(c, 0, sum_of_arr)\n",
        "\n",
        "def DeviceHost_new(a, c):\n",
        "    dev_a = cuda.to_device(a)\n",
        "    dev_c = cuda.to_device(c)\n",
        "    start1_1 = time.time()\n",
        "    dev_vec_sum_new[threads_per_block, threads_per_block](dev_a, dev_c)\n",
        "    end1_1 = time.time() - start1_1\n",
        "    print(\"Расчёты на GPU без учёта копирования результатов на хост: \")\n",
        "    print(end1_1)\n",
        "    print(f\"При размере вектора: {len(a)}\")\n",
        "    print('----------------------------------------------------------')\n",
        "    c = dev_c.copy_to_host()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJwJMgPbxLhd"
      },
      "source": [
        "def result_gpu_no_hostcopy(my_a):\n",
        "  for i in range(len(my_a)):\n",
        "    DeviceHost_new(my_a[i], c1)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zteAUM2Jy54b",
        "outputId": "9ff4e46d-924c-47b8-ec01-052fb5ecb1e1"
      },
      "source": [
        "result_gpu_no_hostcopy(a_all)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Расчёты на GPU без учёта копирования результатов на хост: \n",
            "0.0004341602325439453\n",
            "При размере вектора: 1000\n",
            "----------------------------------------------------------\n",
            "Расчёты на GPU без учёта копирования результатов на хост: \n",
            "0.0004918575286865234\n",
            "При размере вектора: 10000\n",
            "----------------------------------------------------------\n",
            "Расчёты на GPU без учёта копирования результатов на хост: \n",
            "0.00042939186096191406\n",
            "При размере вектора: 100000\n",
            "----------------------------------------------------------\n",
            "Расчёты на GPU без учёта копирования результатов на хост: \n",
            "0.00026035308837890625\n",
            "При размере вектора: 500000\n",
            "----------------------------------------------------------\n",
            "Расчёты на GPU без учёта копирования результатов на хост: \n",
            "0.0003619194030761719\n",
            "При размере вектора: 1000000\n",
            "----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs9j9b6HwYCU"
      },
      "source": [
        "**ПРОВЕРКА СООТВЕТСТВИЯ РАСЧЁТОВ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf61ZMC_tiZl"
      },
      "source": [
        "def check_value(x1, x2):\n",
        "  if x1 == x2:\n",
        "    print(True)\n",
        "  else:\n",
        "    print(False)\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRgOZS2UwxlL",
        "outputId": "bad859c2-ca60-4941-e472-4a4a7da188fd"
      },
      "source": [
        "check_value(DeviceHost(a1, c1), cpu_vec_sum(a1))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPr0lF30x7YV"
      },
      "source": [
        "**СРАВНИТЕЛЬНАЯ ТАБЛИЦА**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLdEZ5h9ykMN"
      },
      "source": [
        "def result_gpu_time(my_a):\n",
        "  end_arr = []\n",
        "  for i in range(len(my_a)):\n",
        "    start = time.time()\n",
        "    DeviceHost(my_a[i], c1)\n",
        "    end = time.time() - start\n",
        "    end_arr.append(end)\n",
        "  return end_arr\n",
        "\n",
        "r1 = result_gpu_time(a_all)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN1sl5EnzX3b"
      },
      "source": [
        "def result_cpu_time(my_a):\n",
        "  end_arr = []\n",
        "  for i in range(len(my_a)):\n",
        "    start = time.time()\n",
        "    cpu_vec_sum(my_a[i])\n",
        "    end = time.time() - start\n",
        "    end_arr.append(end)\n",
        "  return end_arr\n",
        "\n",
        "r2 = result_cpu_time(a_all)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-_BpPKg0hg1"
      },
      "source": [
        "@cuda.jit\n",
        "def dev_vec_sum_new_time(a, c):\n",
        "  sh_b = cuda.shared.array(threads_per_block, int32)\n",
        "\n",
        "  thread_id = cuda.threadIdx.x\n",
        "  block_id = cuda.blockIdx.x\n",
        "  idx = thread_id + block_id * threads_per_block\n",
        "  sh_b[thread_id] = 0\n",
        "\n",
        "  if idx < a.shape[0]:\n",
        "    sh_b[thread_id] = a[idx]\n",
        "    cuda.syncthreads()\n",
        "    if cuda.threadIdx.x == 0:\n",
        "      sum_of_arr = 0\n",
        "      for i in range(threads_per_block):\n",
        "        sum_of_arr += sh_b[i]\n",
        "      cuda.atomic.add(c, 0, sum_of_arr)\n",
        "\n",
        "def DeviceHost_new_time(a, c):\n",
        "    dev_a = cuda.to_device(a)\n",
        "    dev_c = cuda.to_device(c)\n",
        "    start1_1 = time.time()\n",
        "    dev_vec_sum_new[threads_per_block, threads_per_block](dev_a, dev_c)\n",
        "    end1_1 = time.time() - start1_1\n",
        "    return end1_1  "
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wewlxS7b0_ub"
      },
      "source": [
        "def result_cpu_time_nohost(my_a):\n",
        "  end_arr = []\n",
        "  for i in range(len(my_a)):\n",
        "    end = DeviceHost_new_time(my_a[i], c1)\n",
        "    end_arr.append(end)\n",
        "  return end_arr\n",
        "\n",
        "r3 = result_cpu_time_nohost(a_all)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4-KgGfb3uTN"
      },
      "source": [
        "**ИТОГ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "kJLWGCNWx-vT",
        "outputId": "05d5153b-584c-411c-b000-809533dd402c"
      },
      "source": [
        "compar_pd = pd.DataFrame({'GPU': [r1[0], r1[1], r1[2], r1[3], r1[4]], 'CPU': [r2[0], r2[1], r2[2], r2[3], r2[4]], 'GPU no host': [r3[0], r3[1], r3[2], r3[3], r3[4]] }, index = [x1, x2, x3, x4, x5] )\n",
        "compar_pd"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GPU</th>\n",
              "      <th>CPU</th>\n",
              "      <th>GPU no host</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>0.002635</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000</th>\n",
              "      <td>0.005284</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100000</th>\n",
              "      <td>0.002320</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500000</th>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.002563</td>\n",
              "      <td>0.000288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000000</th>\n",
              "      <td>0.002601</td>\n",
              "      <td>0.003093</td>\n",
              "      <td>0.000250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              GPU       CPU  GPU no host\n",
              "1000     0.002635  0.000073     0.000832\n",
              "10000    0.005284  0.000049     0.000269\n",
              "100000   0.002320  0.000999     0.000299\n",
              "500000   0.002014  0.002563     0.000288\n",
              "1000000  0.002601  0.003093     0.000250"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywJAHGdO1p7U"
      },
      "source": [
        "Итог: расчёты по GPU оказались медленнее, чем расчёты на CPU, если брать в расчёт время переноса переменной вывода с device на host. Если же не учитывать это время, то при большом размере вектора расчёты на GPU оказываются быстрее, чем расчёты на CPU."
      ]
    }
  ]
}