Задача: реализовать алгоритм сложения элементов вектора
Язык: C++ или Python
Входные данные: Вектор размером 1 000..1 000 000 значений.
Выходные данные: сумма элементов вектора + время вычисления
Реализация должна содержать 2 функции сложения элементов вектора: на CPU и на
GPU с применением CUDA.
Отчет о проделанной лабораторной работе - это git-репозиторий с исходным кодом
реализации + описание проделанной работы там же в readme.
Необходимо описать реализацию, объяснив, что конкретно было распараллелено и
почему.
Провести эксперименты: получить сумму векторов разных размеров (провести 5 или
более экспериментов), посчитать ускорение. Результаты привести в виде
таблицы/графика.

Техническое обеспечение: Графический процессор, использованный в Colab, Nvidia K80s Tesla. Процессор: AMD Ryzen 5 3500U with Radeon Vega Mobile Gfx, 2096 МГц ОЗУ - 8,00 ГБ (доступно: 7,1 ГБ) Система - Windows 10 (Версия 10.0.19042.1237) Язык реализации программы - Python 3.9.2 Описание: На вход функцииям подается 2 матрицы, случайно сгенерированные с помощью np.random.randn с размером 256х256, 512x512, 1024x1024

Реализованы 2 функции в отдельных блоках "google colab".

Был использован компилятор "numba jit", который переводит сколько возможно когда python - в машинный код, тем самым ускоряя процесс выполнения программы.

Для визуализации полученных данных о времени выполнения был реализован датафрейм, в котором указан тип процессора, на котором выполнялся расчёт, за сколько времени он был выполнен и какую размерность имели вектора.

![image](https://user-images.githubusercontent.com/80954194/136904494-b64b5564-1cc5-4e80-9109-208eb79964a9.png)

Также был реализован график зависимости скорости выполнения задачи от размера вектора.

![image](https://user-images.githubusercontent.com/80954194/136907607-63a59cb6-91f2-4cef-9d67-da1fc515f16b.png)

